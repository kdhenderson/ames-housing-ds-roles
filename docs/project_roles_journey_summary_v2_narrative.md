# Ames Housing Project: A Journey Through Data Science Roles
## Kristin Henderson

This project, centered on the Ames Housing dataset, served as an exploration of various interconnected data science roles. The journey highlighted several overarching themes in practical data science work. Collaboration with AI, while powerful, demonstrated that prolonged, continuous interactions can sometimes reduce efficiency, suggesting that breaking tasks into smaller, focused conversations might optimize prompting and interaction speed. Furthermore, the distinct roles often revealed significant overlap, underscoring that real-world project execution is rarely linear; instead, it demands an iterative approach, revisiting and refining work as new insights emerge from different "perspectives" or roles. As the project expanded in complexity with numerous files and defined roles, maintaining meticulous organization and ensuring consistent updates across all documentation became paramount for clarity and effective management. Finally, this phase primarily involved a focused exploration of individual roles, often with a necessarily narrow scope. Consequently, significant opportunities remain for future work, particularly in fully integrating these roles and tasks, such as incorporating newly acquired datasets (e.g., school data) into the modeling pipeline and subsequent, more holistic analyses.

What follows is a summary of the key challenges, learnings, and accomplishments experienced within each defined project role.

## 1. Project Manager

The Project Manager role emphasized the foundational importance of aligning project objectives with stakeholder needs from the very beginning. A key challenge, and thus a significant learning, was the practical application of user-centered project management: the critical step of identifying stakeholders *before* defining objectives, success metrics, and deliverables proved crucial for ensuring project impact and efficiency. This reinforced the value of a clear project roadmap. The primary accomplishment was the development of the initial `project_roadmap.md`, which laid out the project's objectives, identified stakeholders, defined success metrics, and provided a detailed plan for execution.

## 2. Stakeholder Liaison

As a Stakeholder Liaison, the main challenge lay in the broad skill set required—encompassing strong communication, technical understanding, organizational prowess, and meticulous documentation. This experience highlighted the centrality of communication in bridging the gap between technical teams and business or subject matter experts. A key learning was that the liaison's value is significantly magnified in larger, more technically complex projects with diverse stakeholders. It also became clear that this role is vital not just for initial alignment but for ongoing user education, training, and facilitating a continuous feedback loop. The main contribution was developing `stakeholder_liaison.md`, which outlines communication strategies and underscores the pivotal nature of this role in ensuring deliverables meet stakeholder needs.

## 3. Data Engineer

The Data Engineer faced several challenges, including the nuanced task of distinguishing between and correctly encoding ordinal versus nominal categorical features—resolved through deeper analysis and implementing tailored encoding strategies. Another hurdle was the incorrect initial sequencing of new engineered features (like `TotalSF`), which was addressed by carefully debugging the data flow and reordering transformation steps. A significant iterative loop occurred when the ETL pipeline (`src/etl/etl_main.py`) had to be adapted based on evolving modeling requirements from the Data Scientist, specifically the need for integer-coded nominal features for deep learning embedding layers, requiring adjustments to data output formats. Prompt engineering for AI to achieve focused outputs for data engineering tasks, rather than full pipeline scripts, also required iterative refinement, as did ensuring the creation of essential documentation like data dictionaries and schemas. Key learnings included the paramount importance of clear communication, iterative refinement (especially in response to modeling needs), and robust documentation for building a transparent and reliable data pipeline, alongside practical experience in debugging data flows. Accomplishments include the development of `src/etl/etl_main.py`, authoring `src/etl/etl_documentation.md`, `data_dictionary.md`, and `data_schema.json`, and producing the foundational `data/processed/housing_cleaned.csv`.

## 4. Data Acquisition Specialist

The Data Acquisition Specialist encountered challenges primarily due to limitations in the Ames dataset, such as the lack of direct property identifiers (addresses, lat/lon), which made automated school matching difficult; this was approximated by manual neighborhood-to-school mapping. Attempts to scrape school metrics failed due to JavaScript rendering and anti-bot measures, necessitating manual data collection, and reliable neighborhood-level crime data proved difficult to integrate directly. A significant learning was recognizing the necessity of robust data sources (like APIs or official data) or advanced scraping techniques (e.g., browser automation) for scalable data acquisition, and the importance of thoroughly documenting these challenges. Key contributions include acquiring and processing school data (creating `data/processed/neighborhood_schools.csv` and `schools_metrics.csv`), producing `data/processed/schools_combined.csv` for potential integration, and authoring `future_acquisition_plan.md`.

## 5. Data Steward / Governance Officer

For the Data Steward, a unique challenge arose from the project's primary dataset being a single CSV file without inherently sensitive information. This simplicity meant that a governance system was implemented with an eye towards *potential* or future sensitive data, rather than managing complex existing database views or granular access permissions. Key learnings involved clarifying the distinction between the Data Steward role (focused on governance, policy, documentation) and Data Engineers or model-focused risk roles. There was also the development of an understanding of how to design scalable access control, logging, and monitoring systems, even for simple data storage, and recognizing the importance of establishing clear policies for handling sensitive data fields. Accomplishments include implementing `src/governance/access_control.py` and `access_monitor.py`, setting up `data/sensitive/data_access_log.db`, creating user and steward guides, and establishing a plan for responsibly incorporating sensitive data.

## 6. Data Scientist

The Data Scientist's journey involved overcoming initial poor performance of the deep learning model. This was resolved by log-transforming the target variable and, crucially, implementing Embedding layers for nominal features—a breakthrough requiring close collaboration with the Data Engineer to modify `src/etl/etl_main.py` for integer-coded nominals and `src/modeling/utils/data_loader.py` for the new input structure. Debugging the KerasTuner implementation (e.g., a `use_bias=False` issue and incorrect model retrieval) which initially yielded misleading results was another hurdle. Significant learnings included confirming Random Forest's effectiveness as a strong baseline, understanding the dramatic impact of Embedding layers for categorical data in deep learning, and gaining practical experience in systematic hyperparameter tuning and robust K-fold cross-validation, including interpreting variance in K-fold results. Key accomplishments were developing the `random_forest_baseline.py`, iteratively improving deep learning models, and spearheading the essential ETL and data loader modifications to support these advanced architectures.

## 7. Machine Learning Engineer

The Machine Learning Engineer focused on the technical implementation of models. Challenges included setting up a consistent deep learning framework, which involved resolving environment and package compatibility issues (e.g., Python versions, numpy, TensorFlow), managing a transition from a Conda environment to a project-local `venv`, and restructuring project files. Iterative model debugging also influenced data loading; for instance, debugging the deep learning model necessitated adapting data loading mechanisms initially designed for Random Forest to ensure compatibility or isolate issues, highlighting cross-model dependencies. Early deep learning predictions were also consistently low, potentially due to missing data handling or scaling. Key learnings included initial exposure to the Keras Functional API, familiarity with model serialization (e.g., `.h5`, `.joblib`), the critical importance of saving all model artifacts for reproducibility and deployment, and adapting data loading strategies for different models. Accomplishments include successfully training and saving both Random Forest and Deep Learning models and all associated artifacts, and ensuring `requirements.txt` was accurate.

## 8. Front-End Engineer

The Front-End Engineer's primary challenge was ensuring the Gradio app inputs correctly mapped to the trained models' expectations, demanding close collaboration with the ML Engineer. There was also the ongoing consideration of which features are most relevant and user-friendly for an interactive prediction app, recognizing app design as an iterative process that might involve adding visualizations and enhancing overall usefulness. Significant learnings included understanding how model choice impacts user experience and app design, and recognizing that small UI/UX improvements (like human-readable labels and rounded defaults) significantly enhance usability and stakeholder trust. Key contributions were the development of two Gradio apps (`rf_gradio_app.py` and `gradio_app.py`), UI/UX enhancements, and the `docs/gradio_app_guide.md`.

## 9. Visualization Expert / Data Storyteller

The Visualization Expert / Data Storyteller faced challenges in deploying the Gradio app to Hugging Face Spaces, which involved troubleshooting dependency issues and file path configurations. Making complex evaluation metrics like MAPE intuitive for stakeholders was another focus, addressed by adding clear, example-based explanations. Key learnings included practical experience in troubleshooting web app deployment, the importance of consistent styling and clear labeling in visualizations, and how to explain statistical metrics accessibly. A core appreciation was that a key goal of data storytelling is to produce clear, tangible reports (like `docs/model_comparison_report.md`) that document findings, created artifacts, and their current utility for stakeholders, recognizing this often requires iterative refinement. Accomplishments include developing `src/regression_analysis_visualizations.py` for automated plotting, generating numerous figures, integrating them into the model comparison report, and supporting the Gradio app deployment.

## 10. Explainability Engineer

The Explainability Engineer's main challenge was communicating the concepts behind SHAP values and their interpretation to a non-technical audience, which was addressed by creating a dedicated explanation document with analogies. Significant learnings included gaining initial experience in using and interpreting SHAP values to understand model behavior (particularly for tree-based models), the value of combining visual SHAP plots with narrative explanations, and recognizing how SHAP analysis can reveal non-linear feature impacts. Key accomplishments were performing SHAP analysis for the Random Forest model, generating `figures/shap_summary.png` for the report, and authoring `docs/shap_explanation.md` to make SHAP concepts accessible. 